"""Metrics for use when analyzing the results of an EA run.

Basically, these classes allow us to think of an analysis as a
graph of nodes, where there are certain actions we can perform on
each type of data to produce new, processed types of data.

Each class is a thin wrapper around a DataFrame, and we assume that
the raw data comes in a format as generated by LEAP's
FitessStatesCSVProbe operator.
"""
import csv
import os
import shutil
import sys
import tempfile

import click
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

from leap_ec.probe import FitnessStatsCSVProbe


##############################
# Function best()
##############################
def best(df, time_col, metric_cols):
    """Compute the best (highest) fitness value found across a group's 'bsf' attribute."""
    return pd.Series({'best': df[bsf_col].max()})


##############################
# Function auc()
##############################
def auc(df, time_col, metric_cols):
    """A fast area-under-BSF-curve function."""
    df = df[[time_col, bsf_col]].sort_values(time_col)
    df['previous_time'] = [df[time_col].min()] + df[time_col][0:len(df[time_col]) - 1].to_list()
    df['xdiff'] = df[time_col] - df.previous_time
    auc = sum(df.xdiff * df.bsf)
    return pd.Series({'auc': auc})


##############################
# Class CurveAnalyzer
##############################
class CurveAnalyzer():

    def __init__(self, df,
                 time_col: str = FitnessStatsCSVProbe.time_col,
                 metric_cols: list = FitnessStatsCSVProbe.default_metric_cols,
                 experiment_cols: list = (),
                 run_col: str = 'job'):
        assert(df is not None)
        assert(time_col is not None)
        assert(metric_cols is not None)
        assert(len(metric_cols) > 0)
        assert(experiment_cols is not None)
        assert(run_col is not None)
        self.df = df
        self.time_col = time_col
        self.metric_cols = metric_cols
        self.experiment_cols = experiment_cols
        self.run_col = run_col
        assert(time_col in df.columns), f"Expected '{time_col}' column to be in {df.columns}."
        assert(run_col in df.columns), f"Expected '{run_col}' column to be in {df.columns}."
        assert(set(metric_cols).issubset(df.columns)), f"Expected '{metric_cols}' to be a subset of {df.columns}."
        assert(set(experiment_cols).issubset(df.columns)), f"Expected '{experiment_cols}' to be a subset of {df.columns}."

    def avg_curves(self):
        """Return a dataframe that reports the mean and std of each metric for each experimental group,
        averaged over all of the runs.

        This is analogous to scalar_metrics_per_run, except we're aggregating over runs, instead time.
        """
        # We'll be grouping over time and the experiment columns
        group_cols = [self.time_col] + self.experiment_cols

        # Drop the run_col column, since we're aggregating over it
        df_minus_jobs = self.df[group_cols + self.metric_cols]

        # Compute mean and std within each group
        avg_df = df_minus_jobs.groupby(group_cols).agg(['mean', 'std'])

        # Convert the names of the new columns from (metric, mean) to 'metric_mean', etc.
        avg_df.columns = ['_'.join(col).strip() for col in avg_df.columns.values]
        avg_df.reset_index(inplace=True)
        return avg_df

    def scalar_metrics_per_run(self, metrics=(best, auc)):
        """Compute one or more scalar peformance measure of each metric for each run.

        This is analogous to avg_curves, except we're aggregating over time, instead of across runs.
        
        For example, we might use this to take a set of best-so-far curves and reduce
        them to a series convergence times, or area-under-curve score (one value per run)."""

        # We'll be grouping over the run and experiment columns
        group_cols = [self.run_col] + self.experiment_cols

        # Drop the time_col column, since we're aggregating over it
        df_minus_time = self.df[group_cols + self.metric_cols]

        df_scalars = df_minus_time.groupby(group_cols).apply(metrics)
        # df_scalars = df[group + ['eval', 'bsf']].groupby(group).apply(best)
        # df_auc = df[group + ['eval', 'bsf']].groupby(group).apply(auc)
        # df_scalars['auc'] = df_auc.auc
        df_scalars.reset_index(inplace=True)

        # These are the new columns that we've created via aggregation
        scalar_metric_cols = [ f.__name__ for f in metrics ]
        assert(set(group_cols).issubset(df_scalars.columns)), f"Expected {group_cols} to be a subset of {df_scalars.columns}."
        assert(set(scalar_metric_cols).issubset(df_scalars.columns)), f"Expected {scalar_metric_cols} to be a subset of {df_scalars.columns}."
        return df_scalars, scalar_metric_cols

    def avg_scalar_metrics(self, metrics=(best, auc)):
        """
        Aggregate over both runs and time, computing the mean and std of several scalar
        metrics acrossr runs.

        For example, we can use this to compute the average area-under-best-so-far-curve
        for a collection of runs.
        """
        df_scalars, scalar_metric_cols = self.scalar_metrics_per_run(metrics)

        # We'll be grouping over only the experiment columns
        group_cols = self.experiment_cols

        # Drop the time_col column, since we're aggregating over it
        df_minus_jobs = self.df[group_cols + scalar_metric_cols]

        # Compute mean and std of the scalar metrics within each group
        avg_df = df_minus_jobs.groupby(group_cols).agg(['mean', 'std'])

        # Convert the names of the new columns from (metric, mean) to 'metric_mean', etc.
        avg_df.columns = ['_'.join(col).strip() for col in avg_df.columns.values]
        avg_df.reset_index(inplace=True)
        return avg_df


##############################
# Class BSFCurves
##############################
class BSFCurves():
    """Stores data from multiple EA runs (possibly consisting of many different
    algorithm configurations) and provides operations for analyzing them.
    
    One row per algorithm run."""

    def plot(self, title: str, xlabel: str = None, ylabel: str = None):
        """Plot all of the BSF curves in a single image."""
        plt.figure(figsize=(8,6))
        self.df.groupby(self.groupby_cols).plot(x=self.time_col, y=self.bsf_col, ax=plt.gca(), legend=False)
        if xlabel:
            plt.xlabel(xlabel)
        if ylabel:
            plt.ylabel(ylabel)
        plt.title(title)
        plt.show()


##############################
# Class AvgBSFCurves
##############################
class AvgBSFCurves():
    def plot(self, title, error_bars: bool, legend_column, ylim, xlabel: str = None, ylabel: str = None):
        """Plot the mean BSF curves in a single image."""
        plt.figure(figsize=(8,6))
        for name, group_df in self.df.groupby(self.groupby_cols, as_index=False):
            label = group_df[legend_column].iloc[0] if legend_column else name
            if error_bars:
                assert(self.bsf_std_col in self.df.columns), f"Expected '{self.bsf_std_col}' column to be in {df.columns}."
                group_df.plot(x=self.time_col, y=self.bsf_mean_col, yerr=self.bsf_std_col, capsize=4, fmt='o-', ax=plt.gca(), grid='on', ms=10, label=label)
            else:
                group_df.plot(x=self.time_col, y=self.bsf_mean_col, style='o-', ax=plt.gca(), grid='on', label=label)

        if not legend_column:
            plt.gca().get_legend().remove()
            
        if ylim:
            plt.ylim(*ylim)

        if xlabel:
            plt.xlabel(xlabel)
        if ylabel:
            plt.ylabel(ylabel)
        plt.title(title)
        plt.show()


##############################
# Class AvgScalarMetrics
##############################
class AvgScalarMetrics():


    def plot(self, independent_vars: list, metric: str, line=True):
        assert(independent_vars is not None)
        assert(metric in self.metric_cols), f"Unknown metric {metric}.  Allows values are {self.metric_cols}."

        for v in independent_vars:
            cmap = sns.color_palette("coolwarm", as_cmap=True)
            plt.figure()
            x, y = np.array(self.df[v]), np.array(self.df[metric])
            plt.scatter(self.df[v], self.df[metric], c=self.df[metric], cmap=cmap)

            # Linear fit
            if line:
                m,b = np.polyfit(x, y, 1) 
                end_points = np.array([min(x), max(x)])
                plt.plot(end_points, m*end_points+b, '--k')

            plt.title(f"{v}")
            plt.show(block=False)


##############################
# CLI groups
##############################
@click.group()
def cli():
    """Experiments in evolving controllers for simple 2-D robots."""
    pass

@cli.group()
def analyze():
    """Analysis commands."""
    pass


@cli.group()
def plot():
    """Plotting commands."""
    pass


##############################
# Command analyze
##############################
@analyze.command()
@click.argument('files', nargs=-1)
@click.option('--time-col', default='step', type=str, help="Name of column that represent time (ex. generation, step, eval).")
def fitness(files, time_col):
    """
    Analyze a set of bsf-curve files by computing various metrics and averages.

    Outputs four new files in the current directory.
    """

    combined = './fitnesses_combined.csv'

    # Combine files into one big CSV
    with open(combined, 'w') as f:
        cat_csv_files(files, f)
    
    # Add an eval column, computed from pop_size, etc.
    # with tempfile.NamedTemporaryFile() as f:
    #     #add_eval_column(combined, f)
    #     f.flush()
    #     shutil.copy(f.name, combined)


    # Load as a DataFrame
    df = pd.read_csv(combined, skipinitialspace=True)

    # Wrap it in our analysis engine
    runs = BSFCurves(df, time_col=time_col)
    
    # Write out average BSF curves
    avg_bsf_runs = runs.average_bsf_curves()
    avg_bsf_runs.df.to_csv('./avg_bsf_curves.csv', index=False)

    # Write out area-under-curve
    # auc_runs = runs.scalar_metrics()
    # auc_runs.df.to_csv('./auc_by_run.csv', index=False)

    # # Write out average area-under-curve
    # auc_configs = auc_runs.average()
    # auc_configs.df.to_csv('./auc_by_config.csv', index=False)


##############################
# Command plot bsf
##############################
@plot.command()
@click.argument('fitness-file')
@click.option('--title', type=str, default='Best-so-Far Fitness')
@click.option('--time-col', default='step', type=str, help="Name of column that represent time (ex. generation, step, eval).")
def bsf(fitness_file, title, time_col):
    """Plot a single best-of-generation fitness curve from a CSV file."""
    assert(os.path.exists(fitness_file))
    df = pd.read_csv(fitness_file, skipinitialspace=True)
    curves = BSFCurves(df, time_col=time_col)
    curves.plot(title)


##############################
# Command plot avg-bsf
##############################
@plot.command('avg-bsf')
@click.argument('average-bsf-file')
@click.option('--title', type=str, default='Average Best-so-Far Fitness')
@click.option('--error/--no-error', type=bool, default=True)
@click.option('--legend-col', type=str, default=None)
@click.option('--ylim', type=(float, float), default=(None, None))
@click.option('--time-col', default='step', type=str, help="Name of column that represent time (ex. generation, step, eval).")
def avg_bsf(average_bsf_file, title, error, legend_col, ylim, time_col):
    """Plot average fitness curves from an average-fitness CSV file."""
    assert(os.path.exists(average_bsf_file))
    df = pd.read_csv(average_bsf_file, skipinitialspace=True)
    curves = AvgBSFCurves(df, time_col=time_col)
    curves.plot(title, error, legend_col, ylim)


##############################
# Function cat_csvs()
##############################
def cat_csv_files(files: list, stream=sys.stdout):
    """Combine a bunch of CSVs with the same columns into one CSV with a single header.
    
    The result is written to the given stream."""
    header = None
    writer = csv.writer(stream)

    for fname in files:
        with open(fname, 'r') as f:
            reader = csv.reader(f)

            if header is None:
                header = next(reader)
                writer.writerow(header)
            else:
                this_header = next(reader)
                if this_header != header:
                    raise ValueError(f"Inconsistent headers.  First file was:\n{header}\n\nbut found a file with\n{this_header}.")

            for row in reader:
                writer.writerow(row)


##############################
# Entry point
##############################
if __name__ == '__main__':
    cli()